{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnBOLBJkvMrsI0ffsCBPN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehhitvverma/Cyfuture-assignment/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAA742Rq-yJO"
      },
      "outputs": [],
      "source": [
        "1️⃣ Install Dependencies\n",
        "\n",
        "Ensure you have the required libraries:\n",
        "\n",
        "pip install numpy pandas scikit-learn fastapi uvicorn streamlit torch torchvision tensorflow transformers langchain openai nltk spacy joblib polars\n",
        "\n",
        "2️⃣ Load & Preprocess Data\n",
        "\n",
        "We will use a sample fraud transaction dataset from Kaggle/UCI ML Repository.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fraud_transactions.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Selecting relevant features (assuming 'Class' column is 1 for fraud, 0 for normal)\n",
        "features = [col for col in df.columns if col != 'Class']\n",
        "X = df[features]\n",
        "y = df['Class']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save scaler for later use\n",
        "import joblib\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "3️⃣ Train Isolation Forest (Anomaly Detection Model)\n",
        "\n",
        "This unsupervised model detects anomalies based on deviation from normal transactions.\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import joblib\n",
        "\n",
        "# Train Isolation Forest\n",
        "model = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
        "model.fit(X_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, \"fraud_detector.pkl\")\n",
        "\n",
        "print(\"Model trained and saved successfully!\")\n",
        "\n",
        "4️⃣ Train Autoencoder (Deep Learning Anomaly Model)\n",
        "\n",
        "For deep learning-based fraud detection.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def _init_(self, input_dim):\n",
        "        super(Autoencoder, self)._init_()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "autoencoder = Autoencoder(input_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Train model\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = autoencoder(X_train_tensor)\n",
        "    loss = criterion(outputs, X_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Save Autoencoder Model\n",
        "torch.save(autoencoder.state_dict(), \"autoencoder.pth\")\n",
        "print(\"Autoencoder trained and saved!\")\n",
        "\n",
        "5️⃣ Deploy API with FastAPI\n",
        "\n",
        "Create a FastAPI server to serve predictions.\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "# Load trained models\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "isolation_forest = joblib.load(\"fraud_detector.pkl\")\n",
        "\n",
        "# Load Autoencoder\n",
        "input_dim = X_train.shape[1]\n",
        "autoencoder = Autoencoder(input_dim)\n",
        "autoencoder.load_state_dict(torch.load(\"autoencoder.pth\"))\n",
        "autoencoder.eval()\n",
        "\n",
        "# Initialize API\n",
        "app = FastAPI()\n",
        "\n",
        "class TransactionInput(BaseModel):\n",
        "    features: list  # List of numerical transaction features\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(transaction: TransactionInput):\n",
        "    try:\n",
        "        transaction_array = np.array(transaction.features).reshape(1, -1)\n",
        "        transaction_scaled = scaler.transform(transaction_array)\n",
        "\n",
        "        # Isolation Forest Prediction\n",
        "        isolation_pred = isolation_forest.predict(transaction_scaled)\n",
        "        isolation_result = \"Fraudulent\" if isolation_pred[0] == -1 else \"Legitimate\"\n",
        "\n",
        "        # Autoencoder Prediction\n",
        "        with torch.no_grad():\n",
        "            transaction_tensor = torch.tensor(transaction_scaled, dtype=torch.float32)\n",
        "            reconstructed = autoencoder(transaction_tensor)\n",
        "            reconstruction_error = torch.mean((transaction_tensor - reconstructed) ** 2).item()\n",
        "\n",
        "        # Threshold for fraud detection (tune as needed)\n",
        "        autoencoder_result = \"Fraudulent\" if reconstruction_error > 0.1 else \"Legitimate\"\n",
        "\n",
        "        return {\n",
        "            \"IsolationForest Prediction\": isolation_result,\n",
        "            \"Autoencoder Prediction\": autoencoder_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Run API\n",
        "if _name_ == \"_main_\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, workers=4)\n",
        "\n",
        "6️⃣ Create a Streamlit UI\n",
        "\n",
        "A Streamlit app for real-time predictions.\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "st.title(\"Fraudulent Transaction Detector\")\n",
        "\n",
        "# Input fields\n",
        "st.subheader(\"Enter transaction details:\")\n",
        "features = []\n",
        "for i in range(30):  # Assuming 30 features in dataset\n",
        "    value = st.number_input(f\"Feature {i+1}\", min_value=-100.0, max_value=100.0, value=0.0)\n",
        "    features.append(value)\n",
        "\n",
        "# Predict button\n",
        "if st.button(\"Detect Fraud\"):\n",
        "    data = {\"features\": features}\n",
        "    response = requests.post(\"http://localhost:8000/predict\", json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        st.write(f\"Isolation Forest Prediction: *{result['IsolationForest Prediction']}*\")\n",
        "        st.write(f\"Autoencoder Prediction: *{result['Autoencoder Prediction']}*\")\n",
        "    else:\n",
        "        st.error(\"Error processing request.\")\n",
        "\n",
        "7️⃣ Run the Application\n",
        "\n",
        "Start the FastAPI server\n",
        "\n",
        "python fraud_api.py\n",
        "\n",
        "Run the Streamlit UI\n",
        "\n",
        "streamlit run fraud_ui.py\n",
        "\n",
        "Scalability Features\n",
        "\t•\tBatch inference: Modify FastAPI to handle multiple transactions.\n",
        "\t•\tGPU acceleration: Move autoencoder inference to CUDA.\n",
        "\t•\tAsynchronous processing: Use Celery & Redis for distributed prediction.\n",
        "\t•\tKubernetes & Docker: Deploy API at scale.\n",
        "\n",
        "This end-to-end project implements fraud detection with machine learning & deep learning, a FastAPI server, and a Streamlit UI.\n",
        "\n",
        "Would you like batch inference or Docker deployment?"
      ]
    }
  ]
}